{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: \n",
    "\n",
    "#### PEFT\n",
    "PEFT is used to leverage large models that have previously been trained. It achieves this by updating only a small set of parameters, thus reducing computational resources. The small set of parameters could be the last layer in a multilevel neural network. \n",
    "\n",
    "A version of PEFT, Low Rank Adaptation or Loram, will be used in this example. This technique creates a 'delta' layer using fewer parameters than the original layer. \n",
    "#### Model\n",
    "The model chosen is one of the transformer models - AutoModelForSequenceClassification - which is one of the text sentiment models. It contains only two features, text and label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800c2085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.0\n",
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "print(transformers.__version__)\n",
    "!python --version\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "#from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig\n",
    "from peft import LoraConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b2322",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb1b4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7268b136620e4b4d818d5db55edc2c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 23.7MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 28.3MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:01<00:00, 38.0MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a1a44cd5ba4256ae1097c2b65c567b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbbf4914f434ce080df40cf72901555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe1949dca6b4f1d82101150185b5353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,get_dataset_split_names,load_dataset_builder\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset_builder = load_dataset_builder(\"imdb\")\n",
    "#dataset_builder.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc60668",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\",\"test\"]\n",
    "dataset = {split: dataset for split, dataset in zip(splits,load_dataset(\"imdb\",split=splits))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3733df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    dataset[split] = dataset[split].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89cdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8f2c8",
   "metadata": {},
   "source": [
    "### Dataset Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92a6bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676d6eced4664d0ebb5fca3ec894d3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d359426d7dc24db38d873aa726ed84b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d088d8fcf444436f8c998d0464c730af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde1519886894eeeafa7c6da814eac04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c926aa1b2f48cda1ac221968d1ed61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_type = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ef78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=\"max_length\",truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036947ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=[\"train\", \"test\"]\n",
    "tokenized_ds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9035c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86beb6648afa4ef5bc2067dd51944d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf6dcade1624ae8a396bd7ad7ae0100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3081ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd559a",
   "metadata": {},
   "source": [
    "\n",
    "### Define Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0ddfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    #logits, labels = p.predictions, p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "       # \"eval_loss\": (predictions == labels).mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f7a2a",
   "metadata": {},
   "source": [
    "### Load PreTrained Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351477b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3a7bab57c442acb1993bea9753ea00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_type, num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "modelName = \"GPT2ForSequenceClassification\"\n",
    "runDescription=\"Original Pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243619a1",
   "metadata": {},
   "source": [
    "### Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a5f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model,modelName,runDescription):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        \n",
    "    )\n",
    "    data = {'Model':modelName,'Description':runDescription,'Trainable Parameters': trainable_params,'All Parameters':all_param,'Trainable%': 100 * trainable_params / all_param}\n",
    "    df_params = pd.DataFrame(data, index=[0])\n",
    "    return df_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab92d042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124441344 || all params: 124441344 || trainable%: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "      <th>All Parameters</th>\n",
       "      <th>Trainable%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Original Pretrained</td>\n",
       "      <td>124441344</td>\n",
       "      <td>124441344</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model          Description  Trainable Parameters  \\\n",
       "0  GPT2ForSequenceClassification  Original Pretrained             124441344   \n",
       "\n",
       "   All Parameters  Trainable%  \n",
       "0       124441344       100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = print_trainable_parameters(model,modelName,runDescription)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95675b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenized_ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a58749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze the layers of the pre-trained model to prevent their weights from being updated during fine-tuning\n",
    "for param in model.transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46548d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1536 || all params: 124441344 || trainable%: 0.00123431646639882\n"
     ]
    }
   ],
   "source": [
    "runDescription=\"Requires_Grad = False\"\n",
    "df2=print_trainable_parameters(model,modelName,runDescription) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a157489",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0538605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding, TrainingArguments\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c99bcf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.485632</td>\n",
       "      <td>0.753000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.7853065795898437, metrics={'train_runtime': 80.4306, 'train_samples_per_second': 12.433, 'train_steps_per_second': 3.108, 'total_flos': 267126934339584.0, 'train_loss': 0.7853065795898437, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07870c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1536 || all params: 124441344 || trainable%: 0.00123431646639882\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "      <th>All Parameters</th>\n",
       "      <th>Trainable%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Original Pretrained</td>\n",
       "      <td>124441344</td>\n",
       "      <td>124441344</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Requires_Grad = False</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Trained</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model            Description  Trainable Parameters  \\\n",
       "0  GPT2ForSequenceClassification    Original Pretrained             124441344   \n",
       "0  GPT2ForSequenceClassification  Requires_Grad = False                  1536   \n",
       "0  GPT2ForSequenceClassification                Trained                  1536   \n",
       "\n",
       "   All Parameters  Trainable%  \n",
       "0       124441344  100.000000  \n",
       "0       124441344    0.001234  \n",
       "0       124441344    0.001234  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runDescription=\"Trained\"\n",
    "df3=print_trainable_parameters(model,modelName,runDescription)\n",
    "df_123 = pd.concat([df1, df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b018a901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_eval_data = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fccae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame(full_eval_data, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b64f5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485632</td>\n",
       "      <td>0.753</td>\n",
       "      <td>36.7625</td>\n",
       "      <td>27.202</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0   0.485632          0.753       36.7625                   27.202   \n",
       "\n",
       "   eval_steps_per_second  epoch  \n",
       "0                    6.8    1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dcf583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = df_full.iloc[0, 1]  # iloc[row_index, column_index]\n",
    "print(eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c30837c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "0  There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...   \n",
       "1                                                                                     This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.   \n",
       "\n",
       "   predictions  labels  \n",
       "0            1       1  \n",
       "1            1       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_for_manual_review = tokenized_ds[\"train\"].select(\n",
    "range(2))\n",
    "\n",
    "results = trainer.predict(items_for_manual_review)\n",
    "df = pd.DataFrame(\n",
    "{\n",
    "    \"text\":[item[\"text\"] for item in items_for_manual_review],\n",
    "    \"predictions\": results.predictions.argmax(axis=1),\n",
    "    \"labels\":results.label_ids,\n",
    "    \n",
    "})\n",
    "pd.set_option(\"display.max_colwidth\",None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "849593e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a92cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model, save_model\n",
    "save_model(model, \"model.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985d3b0",
   "metadata": {},
   "source": [
    "### PEFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19d44e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6aa0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=2,           # Rank of the low-rank matrices\n",
    "    lora_alpha=32, # Scaling factor for LoRA\n",
    "    target_modules=[\"score\"], # Target the classification head\n",
    "    lora_dropout=0.1, # Dropout probability for LoRA\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "# Wrap the model with PEFT\n",
    "peft_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3421e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6152 || all params: 124445960 || trainable%: 0.004943511223666883\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "      <th>All Parameters</th>\n",
       "      <th>Trainable%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Original Pretrained</td>\n",
       "      <td>124441344</td>\n",
       "      <td>124441344</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Requires_Grad = False</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Trained</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peft_model</td>\n",
       "      <td>peft without training</td>\n",
       "      <td>6152</td>\n",
       "      <td>124445960</td>\n",
       "      <td>0.004944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model            Description  Trainable Parameters  \\\n",
       "0  GPT2ForSequenceClassification    Original Pretrained             124441344   \n",
       "0  GPT2ForSequenceClassification  Requires_Grad = False                  1536   \n",
       "0  GPT2ForSequenceClassification                Trained                  1536   \n",
       "0                     peft_model  peft without training                  6152   \n",
       "\n",
       "   All Parameters  Trainable%  \n",
       "0       124441344  100.000000  \n",
       "0       124441344    0.001234  \n",
       "0       124441344    0.001234  \n",
       "0       124445960    0.004944  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runDescription=\"peft without training\"\n",
    "modelName = \"peft_model\"\n",
    "df4=print_trainable_parameters(peft_model,modelName,runDescription)\n",
    "df_1234 = pd.concat([df1, df2, df3,df4])\n",
    "display(df_1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd768d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model, save_model\n",
    "\n",
    "save_model(peft_model, \"model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "peft_training_args=TrainingArguments(\n",
    "    output_dir=\"./data1\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    )\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset= tokenized_ds['train'].rename_column('label', 'labels'),\n",
    "    eval_dataset= tokenized_ds['test'].rename_column('label', 'labels'),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e9da00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459315</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.5472578735351562, metrics={'train_runtime': 79.8875, 'train_samples_per_second': 12.518, 'train_steps_per_second': 3.129, 'total_flos': 267141431090688.0, 'train_loss': 0.5472578735351562, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9a32e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_eval_data = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade923dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_model.save_pretrained('model/peft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e081aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(peft_eval_data))\n",
    "df_peft = pd.DataFrame(peft_eval_data, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5048dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6152 || all params: 124445960 || trainable%: 0.004943511223666883\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "      <th>All Parameters</th>\n",
       "      <th>Trainable%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Original Pretrained</td>\n",
       "      <td>124441344</td>\n",
       "      <td>124441344</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Requires_Grad = False</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2ForSequenceClassification</td>\n",
       "      <td>Trained</td>\n",
       "      <td>1536</td>\n",
       "      <td>124441344</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peft_model</td>\n",
       "      <td>peft without training</td>\n",
       "      <td>6152</td>\n",
       "      <td>124445960</td>\n",
       "      <td>0.004944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peft_model</td>\n",
       "      <td>peft with Training</td>\n",
       "      <td>6152</td>\n",
       "      <td>124445960</td>\n",
       "      <td>0.004944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model            Description  Trainable Parameters  \\\n",
       "0  GPT2ForSequenceClassification    Original Pretrained             124441344   \n",
       "0  GPT2ForSequenceClassification  Requires_Grad = False                  1536   \n",
       "0  GPT2ForSequenceClassification                Trained                  1536   \n",
       "0                     peft_model  peft without training                  6152   \n",
       "0                     peft_model     peft with Training                  6152   \n",
       "\n",
       "   All Parameters  Trainable%  \n",
       "0       124441344  100.000000  \n",
       "0       124441344    0.001234  \n",
       "0       124441344    0.001234  \n",
       "0       124445960    0.004944  \n",
       "0       124445960    0.004944  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelName = \"peft_model\"\n",
    "runDescription = \"peft with Training\"\n",
    "df5=print_trainable_parameters(peft_model,modelName,runDescription)\n",
    "df_12345 = pd.concat([df1, df2, df3,df4,df5])\n",
    "display(df_12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bf20033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both = pd.concat([df_full, df_peft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1907fc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485632</td>\n",
       "      <td>0.753</td>\n",
       "      <td>36.7625</td>\n",
       "      <td>27.202</td>\n",
       "      <td>6.800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.459315</td>\n",
       "      <td>0.770</td>\n",
       "      <td>36.5088</td>\n",
       "      <td>27.391</td>\n",
       "      <td>6.848</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0   0.485632          0.753       36.7625                   27.202   \n",
       "0   0.459315          0.770       36.5088                   27.391   \n",
       "\n",
       "   eval_steps_per_second  epoch  \n",
       "0                  6.800    1.0  \n",
       "0                  6.848    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15700a1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Freezing the pre-trained transformer model, GPT2ForSequenceClassification, reduced the trainable parameters from 124,441,344 to 1536. Using a LORA config with PEFT increased this to 6152. This may be due to the additional low-rank matrices introduced by LoRA layers.  \n",
    "\n",
    "There was also no improvement in accuracy or runtime. Increasing the epocs or sample size also had no discernable improvement with PEFT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817e86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
